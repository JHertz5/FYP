\documentclass[12pt]{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color, soul} 	%for highlighting
\usepackage{listings} 		%for writing code
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}
\usepackage[margin=1in]{geometry} %for margins
\setlength{\parskip}{12pt plus4mm minus3mm} %for paragraph spacing

\setcounter{page}{1}
\begin{document}

\begin{titlepage}

\title{Space Brain - Investigating the Suitability of an FPGA-based Convolutional Neural Network for Space Applications}
\author{Jukka (Jacobus) Hertzog}
\def\supervisor{Dr. Felix Winterstein}
\def\secondmarker{Dr. David Thomas}
\def\course{EE4T}
\def\cid{00828711}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}
\fontfamily{phv}\selectfont
{
\large
\raggedright
Imperial College London\\[17pt]
Department of Electrical and Electronic Engineering\\[17pt]
Final Year Project Interim Report 2017\\[17pt]
}
\rule{\columnwidth}{3pt}
\vfill
\centering
\makeatletter
\begin{tabular}{p{40mm}p{\dimexpr\columnwidth-40mm}}
Project Title: & \textbf{\@title} \\[12pt]
Student: & \textbf{\@author} \\[12pt]
CID: & \textbf{\cid} \\[12pt]
Course: & \textbf{\course} \\[12pt]
Project Supervisor: & \textbf{\supervisor} \\[12pt]
Second Marker: & \textbf{\secondmarker} \\
\end{tabular}
\end{titlepage}

\section{Introduction}
\label{sec:Intro}

Inspired by the structure of the optical nervous system in animals, a neural network is made up of layers of artificial neurons that recognise certain features from the input data. The idea of an artificial neural network was first introduced in 1980\cite{neocognitron}, and driven by increases in computing power and a growth in machine learning applications, neural networks have become very powerful tools. In the last decade, the development of Convolutional Neural Networks (CNNs) has lead to great advancements in image classification accuracy.

CNNs are a powerful Deep Learning tool that can be used to solve extremely complex computational problems. In particular, they have gained popularity in image classification applications \cite{ImageNetChallenge}. Other popular applications within machine vision include video classification, face detection and gesture recognition. They are also being used in a wide range of other fields including speech recognition, natural language processing and text classification.

However, whilst achieving exceptional performance, CNNs are extremely computation and resource heavy. Typically, they will be implemented on large servers or on GPU based systems to accomodate the need for computational power. As a result, implementing them on embedded systems, which typically have very limited resources, presents many challenges. A promising solution to this problem is the use of an FPGA, which provide a very high computational efficiency with low power usage, in addition to other benefits.

Now suppose that the FPGA based CNN will be implemented on a sattelite. In orbit, the satellite will be exposed to intense raditiation, and as a result, errors will be produced on the FPGA. Before such a satellite could be designed and launched, it is important to know how these errors would affect the performance of the CNN, and if the CNN could be designed in such a way that could mitigate the impact of these errors. Therefore, the aim of this project is to implement a CNN on an FPGA-based system, and then to investigate the suitability of this system for space applications.

\subsection{Report Structure}
\label{sec:Intro-Structure}

The rest of the report is organised as follows. Section \ref{sec:ProjSpec} will lay out the project specification, and the motivation for the project. Section \ref{sec:Background} will describe the key concepts underlying the project. Section \ref{sec:ImpPlan} will go into detail about how the project will be implemented, including a rough time line. Finally, section \ref{sec:EvalPlan} will describe the metrics through which the project will be evaluated.

\section{Background}
\label{sec:Background}

\subsection{Target Platform}
\label{sec:Background-TargetPlatform}

The hardware used in this project is a product from Xilinx called a Zedboard, a development board for the Zynq-7000 System On a Chip (SoC). This board was chosen because of its useful hardware features and its integrated support for the Xilinx High Level Synthesis (HLS) tools. The Zynq-7000 consists of a Xilinx Artix-7 FPGA as its Programmable Logic (PL), and a dual-core Cortex-A9 ARM processor as its Processing System (PS) integrated together. The ARM processor facilitates easy control of the system, whilst the FPGA will be able to efficiently perform heavy computations.

The system will be designed using Xilinx's SDSoc development environment. SDSoc is able to optimise and compile C, C++ or OpenCL source code for a Zynq system. The compiler generates software for the ARM core and, using an HLS tool, a bitstream for the FPGA. This allows the user to design the entire system, easily accelerating functions with the FPGA. SDSoc is a very powerful tool, facilitating the rapid development and design of firmware.

\subsection{Convolutional Neural Networks}
\label{sec:Background-CNN}

CNNs can be used to classify images in a forward inference process. But before using the CNN for any task, one should first train the CNN on a set of training data. The training of a CNN is usually implemented on large servers with a huge computational capacity, with an enormous set of training data. For the embedded FPGA platform, we only focus on accelerating the inference process of a CNN, and not on the training process.

A typical CNN consists of a number of layers that run in sequence. The parameters of a CNN model are called weights and are set by the training process. These wieghts will determine what each layer of the CNN will be activated by, and ultimately, what the CNN is able to recognise and classify. Convolution (CONV), activation function, pooling, and Fully Connected (FC) layers make up a typical CNN model, with CONV and FC being the most important.

The first layer of a CNN reads an input image and outputs a series of feature maps. The input image will be three dimensional - the height and width of the image make up two dimensions, and colours make up the third layer. For an RGB image, the input will have a depth of 3. Then there will be CONV layers interspersed by activation function and pooling layers, which will make up most of the CNN. These layers will decompose the image into feature maps, varying from low-level features such as edges, lines, curves, etc., in the initial layers to high-leve features in the deeper layers\cite{fpgaCnnAccelerator}. Each subsequent layer reads the feature maps generated by preceding layers and generates new feature maps at its output. Finally a classifier, consisting of at least one FC layer, reads the final feature maps and determines the probability of the input imaging belonging to each category of the training data. A example of a CNN model is shown in Figure \ref{fig:typicalCNN}.
\begin{figure}

\centering
\includegraphics[width=0.75\textwidth]{../figures/typicalCnn}

  \caption{A typical CNN structure from the feature map perspective \cite{embeddedFpgaCnn} \label{fig:typicalCNN}}

\end{figure}

\subsubsection{Convolution Layer}
\label{sec:Background-CNN-Conv}

The CONV layer takes a series of feature maps as input and convolves with convolutional kernels to obtain the output feature maps. It involves 3-dimensional multiply and accumulate operation of $N_{if}$ input features with $K\times K$ convolution kernels to get an output feature neuron value as shown in Equation \ref{eq:ConvLayer}
\begin{equation}
out(f_o,x,y)=\sum^{N_{if}}_{f_i=0} \sum^{K}_{k_x=0} \sum^{K}_{k_y=0} wt(f_o,f_i,k_x,k_y)\times in(f_i,x+k_x,y+k_y)
\label{eq:ConvLayer}
\end{equation}
where $out(f_o,x,y)$ and $in(f_i,x,y)$ represent the neurons at position $(x,y)$ in the feature maps $f_o$ and $f_i$, respectively and $wt(f_o,f_i,k_x,k_y)$ is the weights at position $(k_x,k_y)$ that gets convolved with input feature map $f_i$ to get the output feature map $f_o$\cite{fpgaCnnAccelerator}.

\subsubsection{Activation Functions}
\label{sec:Background-CNN-Activation}

CONV layers are followed by an activation functon layer. This layer can be thought of as a decision, based on the output of the CONV layer, on to what extent each neuron in the next layer has been activated. The CONV layer output is a linear combination of the inputs and the weights at a position in the network, and role of the activation function is to produce a non-linear decision boundary. The commonly used activation functions in traditional neural networks are non-linear functions such as tanh and sigmoid, which require a longer training time in CNNs \cite{AlexNet}. Hence, Rectified Linear Unit (ReLU), defined as $y = max(x,0)$, has become the popular activation function among CNN models as it converges faster in training, and has less computational complexity than exponent functions in tanh and sigmoid, also aiding hardware design.

\subsubsection{Pooling Layer}
\label{sec:Background-CNN-Pool}

Spatial pooling or subsampling is utilized to reduce the feature dimensions as we traverse deeper into the network. As shown in Equation \ref{eq:Pool}, pooling computes the maximum or average of neighboring $K\times K$ neurons in the same feature map, which also provides a form of translational invariance\cite{PoolAnalysis}. The pooling layer helps abstract higher-level features without redundancy, as well as reducing the dimensionality of lower-level features without losing important information\cite{fpgaCnnAccelerator}.
\begin{equation}
out(f_o,x,y)=\underset{0\geqslant (k_x,k_y)<K}{max/average}(in(f_o,x+k_x,y+k_y))
\label{eq:Pool}
\end{equation}

\subsubsection{Fully Connected Layer}
\label{sec:Background-CNN-FC}

An FC layer is a classification layer where all the input features $(N_{if})$ are connected to all of the output features $(N_{of})$ through synaptic weights $(wt)$. These are at the end of the CNN and perform the final classification, based on the features that have been recognised by the rest of the network. Each output neuron is the weighted summation of all the input neurons as shown in Equation \ref{eq:FullyConnected}\cite{fpgaCnnAccelerator}.
\begin{equation}
out(f_o)=\sum^{N_{if}}_{f_i=0}wt(f_o,f_i)\times in(f_i)
\label{eq:FullyConnected}
\end{equation}
The outputs of the inner-product layer traverse through ReLU based activation function to the next inner-product layer or directly to a Softmax function that converts them to probability in the range (0, 1). The final accuracy layer compares the labels of the top probabilities from softmax layer with the actual label and gives the accuracy of the CNN model\cite{fpgaCnnAccelerator}.

\subsection{CNNs Implemented on FPGAs}
\label{sec:Background-CNNsImplementedOnFPGAs}

CNNs are computationally demanding and consume a lot of resources, and thus are difficult to implement on an embedded systems. FPGAs are the most promising platform for the implementation of an embedded CNN, due to their high computational efficiency and low power consumption. FPGAs are also attractive because of their reconfigurability and fast development time, aided by HLS tools from FPGA vendors, which can interpret the desired behaviour of a programming language like C or C++, and generate a digital hardware implementation. Moreover, FPGAs provide flexibility to implement the CNNs with limited data precision which reduces the memory footprint and bandwidth requirements, resulting in a better energy efficiency\cite{fpgaCnnAccelerator}.

Most research has focussed on using hardware to accelerating the computation of the CONV layers. These layers, with their  typically make up around 90\% of the CNN's computational workload.

\subsection{FPGAs and Space Applications}
\label{sec:Background-FPGAsAndSpaceApplications}

Single Event Upsets (SEUs).

\section{Project Specification}
\label{sec:ProjSpec}

The first deliverable is the implementation of a CNN on an embedded platform.  The processor, with it's easily developed code, will control the system, while the FPGA, being able to efficiently perform heavy computations, will be used to accelerate functions. The end result will be a fully functional CNN that runs on the Zedboard and is able to process and classify images with a reasonable execution time and accuracy, compared to implementations from literature discussed in section \ref{sec:Background-CNNsImplementedOnFPGAs}. \hl{possibly different section?}

The second deliverable of this project is the invesitgation of the system's fault tolerance against SEUs. As discussed in section \ref{sec:Background-FPGAsAndSpaceApplications}, SEUs from are one of the main challenges to be overcome when using SRAM based FPGAs for applications in space. The system's resistance to SEUs should indicate whether or not the system might be suitable as an on board processor for a satellite in orbit.

\section{Implementation Plan}
\label{sec:ImpPlan}

\subsection{Infrastrucure Setup}
\label{sec:ImpPlan-InfSetup}

This first part of the project implementation will be setting up two important tools that will be vital whilst working towards the first deliverable.

The other important tool is Caffe (or Convolutional Architecture for Fast Feature Embedding). This is a deep learning framework that can be used to design, train, optimise and test CNNs, with a focus on computer vision\cite{jia2014caffe}. It also includes some examples of pre-trained CNN models that are ready for use. Using Caffe, it will be possible to generate C++ code for a CNN, which can then be implemented on the Zynq system using SDSoc.

\subsection{FPGA Implementation of a CNN}
\label{sec:ImpPlan-FPGAImplOfCnn}



\subsection{Fault Injection Experiments}
\label{sec:ImpPlan-FaultInjExp}

\section{Evaluation Plan}
\label{sec:EvalPlan}

These SEUs can be simulated with a process called fault injection \cite{FaultInjection}. These will 

\bibliography{interimReport_ref}
\bibliographystyle{ieeetr}
\nocite{*}


\end{document}